# JAVA

## 线程

### 线程的生命周期

1. 基本介绍
一个线程完整的生命周期中通常要经历如下的五种状态：
① 新建： 当一个Thread类或其子类的对象被声明并创建时，新生的线程对象处于新建状态。
② 就绪：处于新建状态的线程被start()后，将进入线程队列等待CPU时间片，此时它已具备了运行的条件。
③ 运行：当就绪的线程被调度并获得处理器资源时,便进入运行状态， run()方法定义了线程的操作和功能。
④ 阻塞：在某种特殊情况下，被人为挂起或执行输入输出操作时，让出 CPU 并临时中止自己的执行，进入阻塞状态。
⑤ 死亡：线程完成了它的全部工作或线程被提前强制性地中止。  

![image-20200723130026095](C:\Users\swime\AppData\Roaming\Typora\typora-user-images\image-20200723130026095.png)



![image-20200723130040159](C:\Users\swime\AppData\Roaming\Typora\typora-user-images\image-20200723130040159.png)

### 线程，协程，进程区别

#### 进程

```
进程：上级挂靠的单位是操作系统，系统会以进程为单位，分配系统资源（CPU时间片，内存等资源），进程是资源分配的最小单位。

进程间通讯（ipc）
管道(Pipe)、命名管道(FIFO)、消息队列(Message Queue) 、信号量(Semaphore) 、共享内存（Shared Memory）；套接字（Socket）。
```

![image-20200723235351020](C:\Users\swime\AppData\Roaming\Typora\typora-user-images\image-20200723235351020.png)



#### 线程

```
线程，有时被称为轻量级进程(Lightweight Process，LWP），是操作系统调度（CPU调度）执行的最小单位。
```

![image-20200723235401491](C:\Users\swime\AppData\Roaming\Typora\typora-user-images\image-20200723235401491.png)

#### 进程与线程的区别

1. 调度：线程是基本调度和分配单位，进程则是拥有资源的基本单位
2. 并发性：进程线程都可以并发执行。
3. 资源：进程有独立资源，线程没有。
4. 系统开销：创建进程的开销大于线程开销。
5. 健壮性：保护模式下，进程之间是隔离的。而一个进程里多线程运行时，一个线程死掉，等于多个线程死掉。健壮性进程大于线程！

#### 进程与线程的联系

- **一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程**；
- 资源分配给进程，同一进程的所有线程共享该进程的所有资源；
- 处理机分给线程，即**真正在处理机上运行的是线程**；
- 线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步。

## 并发

35. 并行和并发有什么区别？

* 并行：多个处理器或多核处理器同时处理多个任务。
* 并发：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。
如下图：

并发 = 两个队列和一台咖啡机。
并行 = 两个队列和两台咖啡机。
36. 线程和进程的区别？

一个程序下至少有一个进程，一个进程下至少有一个线程，一个进程下也可以有多个线程来增加程序的执行速度。
37. 守护线程是什么？

守护线程是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。在 Java 中垃圾回收线程就是特殊的守护线程。
38. 创建线程有哪几种方式？

创建线程有三种方式：
* 继承 Thread 重写 run 方法；
* 实现 Runnable 接口；
* 实现 Callable 接口。
39. 说一下 runnable 和 callable 有什么区别？

runnable 没有返回值，callable 可以拿到有返回值，callable 可以看作是 runnable 的补充。
40. 线程有哪些状态？

线程的状态：
* NEW 尚未启动
* RUNNABLE 正在执行中
* BLOCKED 阻塞的（被同步锁或者IO锁阻塞）
* WAITING 永久等待状态
* TIMED_WAITING 等待指定的时间重新被唤醒的状态
* TERMINATED 执行完成
* 类的不同：sleep() 来自 Thread，wait() 来自 Object。
* 释放锁：sleep() 不释放锁；wait() 释放锁。
* 用法不同：sleep() 时间到会自动恢复；wait() 可以使用 notify()/notifyAll()直接唤醒。
41. sleep() 和 wait() 有什么区别？

42. notify()和 notifyAll()有什么区别？

notifyAll()会唤醒所有的线程，notify()之后唤醒一个线程。notifyAll() 调用后，会将全部线程由等待池移到锁池，然后参与锁的竞争，竞争成功则继续执行，如果不成功则留在锁池等待锁被释放后再次参与竞争。而 notify()只会唤醒一个线程，具体唤醒哪一个线程由虚拟机控制。
43. 线程的 run() 和 start() 有什么区别？

start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start() 只能调用一次。
44. 创建线程池有哪几种方式？

线程池创建有七种方式，最核心的是最后一种：
* newSingleThreadExecutor()：它的特点在于工作线程数目被限制为 1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目；
* newCachedThreadPool()：它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过 60 秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用 SynchronousQueue 作为工作队列；
* newFixedThreadPool(int nThreads)：重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有 nThreads 个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目 nThreads；
* newSingleThreadScheduledExecutor()：创建单线程池，返回 ScheduledExecutorService，可以进行定时或周期性的工作调度；
* newScheduledThreadPool(int corePoolSize)：和newSingleThreadScheduledExecutor()类似，创建的是个 ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程；
* newWorkStealingPool(int parallelism)：这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序；
* ThreadPoolExecutor()：是最原始的线程池创建，上面1-3创建方式都是对ThreadPoolExecutor的封装。
* RUNNING：这是最正常的状态，接受新的任务，处理等待队列中的任务。
* SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务。
* STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程。
* TIDYING：所有的任务都销毁了，workCount 为 0，线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated()。
* TERMINATED：terminated()方法结束后，线程池的状态就会变成这个。
* execute()：只能执行 Runnable 类型的任务。
* submit()：可以执行 Runnable 和 Callable 类型的任务。
45. 线程池都有哪些状态？

46. 线程池中 submit() 和 execute() 方法有什么区别？

Callable 类型的任务可以获取执行的返回值，而 Runnable 执行无返回值。
47. 在 Java 程序中怎么保证多线程的运行安全？

* 方法一：使用安全类，比如 Java. util. concurrent 下的类。
* 方法二：使用自动锁 synchronized。
* 方法三：使用手动锁 Lock。
手动锁 Java 示例代码如下：
Lock lock = new ReentrantLock();
lock. lock();
try {
    System. out. println("获得锁");
} catch (Exception e) {
    // TODO: handle exception
} finally {
    System. out. println("释放锁");
    lock. unlock();
}
48. 多线程中 synchronized 锁升级的原理是什么？

synchronized 锁升级原理：在锁对象的对象头里面有一个 threadid 字段，在第一次访问的时候 threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。
锁的升级的目的：锁升级是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。
49. 什么是死锁？

当线程 A 持有独占锁a，并尝试去获取独占锁 b 的同时，线程 B 持有独占锁 b，并尝试获取独占锁 a 的情况下，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。
50. 怎么防止死锁？

* 尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。
* 尽量使用 Java. util. concurrent 并发类代替自己手写锁。
* 尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。
* 尽量减少同步的代码块。
51. ThreadLocal 是什么？有哪些使用场景？

ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。
ThreadLocal 的经典使用场景是数据库连接和 session 管理等。
52. 说一下 synchronized 底层实现原理？

synchronized 是由一对 monitorenter/monitorexit 指令实现的，monitor 对象是同步的基本实现单元。在 Java 6 之前，monitor 的实现完全是依靠操作系统内部的互斥锁，因为需要进行用户态到内核态的切换，所以同步操作是一个无差别的重量级操作，性能也很低。但在 Java 6 的时候，Java 虚拟机 对此进行了大刀阔斧地改进，提供了三种不同的 monitor 实现，也就是常说的三种不同的锁：偏向锁（Biased Locking）、轻量级锁和重量级锁，大大改进了其性能。
53. synchronized 和 volatile 的区别是什么？

* volatile 是变量修饰符；synchronized 是修饰类、方法、代码段。
* volatile 仅能实现变量的修改可见性，不能保证原子性；而 synchronized 则可以保证变量的修改可见性和原子性。
* volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞。
* synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。
* synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。
* 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。
54. synchronized 和 Lock 有什么区别？

* synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。
* synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。
* 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。
55. synchronized 和 ReentrantLock 区别是什么？

synchronized 早期的实现比较低效，对比 ReentrantLock，大多数场景性能都相差较大，但是在 Java 6 中对 synchronized 进行了非常多的改进。
主要区别如下：

* ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作；
* ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁；
* ReentrantLock 只适用于代码块锁，而 synchronized 可用于修饰方法、代码块等。
* volatile 标记的变量不会被编译器优化；synchronized 标记的变量可以被编译器优化。
56. 说一下 atomic 的原理？

atomic 主要利用 CAS (Compare And Wwap) 和 volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。

57.volatile关键字的两层语义
1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。
2）禁止进行指令重排序。 
  两层意思：
            ①当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行；
            ②在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。
3）不能保证原子性



#### JVM优化

**![image-20200721193313024](C:\Users\swime\AppData\Roaming\Typora\typora-user-images\image-20200721193313024.png)**



# SCALA

## 隐式转换 

隐式转换类 为了临时补充现有类（官方源碼）的方法实现

```scala

import java.io.File

import scala.io.Source

object ImplicitDemo4 {
    implicit class RichFile(file: File) {
        
        def readContent: String = {
            Source.fromFile(file, "utf-8").mkString
        }
    }
    def main(args: Array[String]): Unit = {
        
        // File => RichFile
        val content = new File("""C:\Users\lzc\Desktop\class_code\2019_11_28\01_scala\scala1128\src\main\scala\com\atguigu\scala1128\day06\implicitdemo\ImplicitDemo4.scala""").readContent
        println(content)
        
    }
    
    
}


```



## 隐式转换方法

```scala
import java.io.{File, FileReader}

import scala.io.Source

/**
 * Author atguigu
 * Date 2020/4/27 9:08
 */
object ImplicitDemo2 {
    def main(args: Array[String]): Unit = {
        // 读取文本文件中的内容  java: IO
        implicit def file2RichFile(file: File) = new RichFile(file)
        
        val content = new File("C:\\Users\\lzc\\Desktop\\class_code\\2019_11_28\\01_scala\\scala1128\\src\\main\\scala\\com\\atguigu\\scala1128\\day06\\implicitdemo\\ImplicitDemo2.scala").readContent
        println(content)
    }
}

class RichFile(file: File) {
    
    // 这个方法要真正的去封装读 文件 内容的代码
    def readContent: String = {
        // Array(1,2,3,4)  => arr.mkString   "1234"
        // Array(1,2,3,4)  => arr.mkString(",")   "1,2,3,4"
        Source.fromFile(file, "utf-8").mkString
    }
}

```



# 

### Kafka为什么快

```
1.零拷贝
2.page cache
3.顺序写
4.分区+分段 索引
5.批量压缩 批量读写
```



### Hive UDF/UDTF/UDAF

```java
UDF
package com.example.hive.udf;
 
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;
 
public final class Lower extends UDF {
  public Text evaluate(final Text s) {
    if (s == null) { return null; }
    return new Text(s.toString().toLowerCase());
  }
}

//
create temporary function my_lower as 'com.example.hive.udf.Lower';

//
hive> select my_lower(title), sum(freq) from titles group by my_lower(title);
 
...
 
Ended Job = job_200906231019_0006
OK
cmo 13.0
vp  7.0
```

```java
UDTF  init process close
public class MyUDTF extends GenericUDTF{
    // 参数校验
    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {

}
    
     //检查类型是否是String类型  string指的是hive中的string类型
        

        // 返回的一行中的每一列的临时字段名
   
        // 声明返回的一行中的每一列的字段类型检测器
        

    // 完成函数的计算逻辑  b(line) 返回   2列N(参数中事件对象的个数)行
    // 参数： [{},{},{}]
    @Override
    public void process(Object[] objects) throws HiveException {

    }

    //最后关闭时，执行操作
    @Override
    public void close() throws HiveException {

    }
}
```



### Spark UDF/UDTF/UDAF

```scala
UDF
 sparkSession.udf.register("idParse",(str:String)=>{
     //注册一个函数，实现case when的函数
    str match{
          case "7" => "id7"
          case "8" => "id8"
          case "9" => "id9"
          case _=>"others"
         }
                        })


```

```scala
UDTF
class MyFloatMap extends GenericUDTF{
   override def close(): Unit = {}
              //这个方法的作用：1.输入参数校验 2. 输出列定义，可以多于1列，相当于可以生成多行多列数据
   override def initialize(args:Array[ObjectInspector]): StructObjectInspector = {
   }

              //这是处理数据的方法，入参数组里只有1行数据,即每次调用process方法只处理一行数据
 override def process(args: Array[AnyRef]): Unit = {
                    
       }
```

```SCALA
UDAF
 class MyAvg extends UserDefinedAggregateFunction{
                //输入数据的类型
override def inputSchema: StructType = 
                //定义输入数据的类型
 override def dataType: DataType = IntegerType
                //规定一致性
override def deterministic: Boolean = true
                //初始化操作
verride def initialize(buffer: MutableAggregationBuffer): Unit = {}

                //map端reduce,所有数据必须过这一段代码
override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {

                }
                //reduce数据，update里面Row，没有第二个字段，这时候就有了第二个字段
 override def merge(buffer: MutableAggregationBuffer, input: Row): Unit = {

                }
                //返回最终结果
override def evaluate(finalVaue: Row): Int = {}
                }
```



#### 业务梳理

##### 电商专用词汇

```
CPT是一种以时间来计费的广告，国内很多的网站都是按照“一个星期多少钱”这种固定收费模式来收费

CPS(cost per sales) 以实际销售产品数量来换算广告刊登金额
CPS是一种以实际销售产品数量来计算广告费用的广告，这种广告更多的适合购物类、导购类、网址导航类的网站，需要精准的流量才能带来转化。

CPD是代表两种含义的收费方式，分别为：Cost Per Download；Cost per day，
可以按照下载量进行收费，也可以按照天数进行收费，根据不同的推广网站


CVR
ROI

（1）CPT和CPM只在第一步收取广告费用，即媒体只需要将广告对广告受众进行了展示，即可向广告商收取广告费用。

（2）CPC只收取第二步费用，消费者看到广告后并进行了点击行为以后，媒体向广告商收取广告费用。

（3）CPA和CPS处于第三步，即消费者有看到广告后并点击了广告，进一步了解活动情况后在广告主的网站完成某些特定行为（例如付款消费，填表注册等）。


广告计算常用指标!
ROI (Return On Investment): 投资回报率 =订单额/消费量（即广告费用）=（单均额*转化量）/（CPA*转化量）=单均额/CPA

CPC (Cost Per Click): 按点击计费（平均点击价格）=消费量/点击量

CPA (Cost Per Action): 按成果数计费 =消费量/转化量=（CPC*点击量）/（CVR*点击量）=CPC/CVR

CPM (Cost Per Mille): 按千次展现计费（千次展现价格）=（消费量/展现量）*1000

CVR (Click Value Rate): 转化率，衡量CPA广告效果的指标 =转化量/点击量

CTR (Click Through Rate): 点击率 =点击量/展现量

PV (Page View): 流量  指网站的是页面浏览量 页面被刷新一次就计算一次。如果网站被刷新了1000次,那PV就是1000 

 uv (Unique Visitor):独立访客，一台电脑24小时以内访问N次计为1次

IP和UV之间的关系：比如，用同一个IP去访问我们的SEO网站，但是一个是用的台式的电脑，一个是用的笔记本，那么网站流量统计工具显示的数据就会是2个UV，1个IP。

ADPV (Advertisement Page View): 载有广告的pageview流量

ADimp (ADimpression): 单个广告的展示次数

PV单价: 每PV的收入，衡量页面流量变现能力的指标

RPS (Revenue Per Search): 每搜索产生的收入，衡量搜索结果变现能力指标

一些词语
SPU SKU
如一件M码（四个尺码：S码、M码、L码、X码）的粉色（三种颜色：粉色、黄色、黑色）Zara女士风衣，其中M码、粉色就是一组SKU的组合
商家进货补货也是通过SKU来完成的，试问淘宝店家跟供货商说我要100件红色女士风衣？供应商知道该怎么给他备货吗？
Zara女士风衣就是SPU
sku : 影响价格和库存的 属性集合, 与商品是多对一的关系，即一个商品有多个SKU。
如流光蓝（三种颜色：流光蓝、霓光紫、霓光渐变色）+8G+128G（两种配置：8G+128G、6G+128G）
Oppo R17有一个SPU、6种SKU
```

Canal MySQL监控  

监控新增数据，过滤出新增用户 可以做实时查看用户新增

可以查看实时PV UV

实时分析新增用户登录平台时方式 手机号/微信号

微信》手机 加强对微信中广告投放

从广告页面跳转

hive业务案例

```
做用户画像
全职家庭主妇

网络模式 wifi 使用系统ios  平均每天累计使用时长大于30分钟
 打开app的时间在正常工作时间内 经纬度 基本无变化 且活跃用户 
 
 统计这类用户消费水平  准确的向内部社交平台推送软广告。
```

HBase rowkey唯一值，不能为空

ES可以用kibana来进行可视化，可以作为可视化的过度使用，但是查询比较麻烦，必须精通ES的算子

ES可以做Hbase的二级索引 通过ES查询内容 反查 id id为Hbase的rowkey

Hbase也可以做到即席查询，可以利用API进行查询，然后将数据封装成json给前端进行展示

离线数仓调优

存储?

执行?

引擎?

每小时，各地区热门商品top10统计（spark/flink）

#### Spark内存管理

##### 堆内内存

```
堆内内存
1.Executor内运行并发任务共享JVM堆内存，这些任务在缓存RDD数据和广播数据时占用的内存被规划为‘存储内存’。
2.而这些任务在执行Shuffle时占用的内存呗规划为执行内存。
3.剩余的部分不做特殊规划, 那些 Spark 内部的对象实例, 或者用户定义的 Spark 应用程序中的对象实例, 均占用剩余的空间.

Spark对堆内内存管理是一种逻辑上的。因为对象实例占用内存的申请和释放都由JVM完成，Spark只能在申请后和释放前记录这些内存。

申请内存流程如下：
1.	Spark 在代码中 new 一个对象实例；
2.	JVM 从堆内内存分配空间，创建对象并返回对象引用；
3.	Spark 保存该对象的引用，记录该对象占用的内存。
释放内存流程如下
1.	Spark记录该对象释放的内存，删除该对象的引用；
2.	等待JVM的垃圾回收机制释放该对象占用的堆内内存。

存在的问题
1.	我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。
2.	对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期。
3.	此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。
4.	虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。

```

##### 堆外内存

```
堆外内存
为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。
堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。
利用 JDK Unsafe API，Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。
堆外内存可以被精确地申请和释放（堆外内存之所以能够被精确的申请和释放，是由于内存的申请和释放不再通过JVM机制，而是直接向操作系统申请，JVM对于内存的清理是无法准确指定时间点的，因此无法实现精确的释放），而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。
在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。
除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。

```

### 志文总结的内存管理

```
execution和storage是Spark Executor中内存的大户，other占用内存相对少很多，这里就不说了。在spark-1.6.0以前的版本，execution和storage的内存分配是固定的，使用的参数配置分别是spark.shuffle.memoryFraction（execution内存占Executor总内存大小，default 0.2）和spark.storage.memoryFraction（storage内存占Executor内存大小，default 0.6），因为是1.6.0以前这两块内存是互相隔离的，这就导致了Executor的内存利用率不高，而且需要根据Application的具体情况，使用者自己来调节这两个参数才能优化Spark的内存使用。在spark-1.6.0以上的版本，execution内存和storage内存可以相互借用，提高了内存的Spark中内存的使用率，同时也减少了OOM的情况。
    在Spark-1.6.0后加入了堆外内存，进一步优化了Spark的内存使用，堆外内存使用JVM堆以外的内存，不会被gc回收，可以减少频繁的full gc，所以在Spark程序中，会长时间逗留再Spark程序中的大内存对象可以使用堆外内存存储。使用堆外内存有两种方式，一种是在rdd调用persist的时候传入参数StorageLevel.OFF_HEAP，这种使用方式需要配合Tachyon一起使用。另外一种是使用Spark自带的spark.memory.offHeap.enabled 配置为true进行使用，但是这种方式在1.6.0的版本还不支持使用，只是多了这个参数，在以后的版本中会开放。
    OOM的问题通常出现在execution这块内存中，因为storage这块内存在存放数据满了之后，会直接丢弃内存中旧的数据，对性能有影响但是不会有OOM的问题。
```



### 志文拳法之Flink

Flink相比传统的Spark Streaming区别?   4个点：处理模型、架构模型、时间机制、容错机制
处理模型：Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。  重点！！！
架构模型：Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。
时间机制：时间机制SparkStreaming支持的时间机制有限，只支持处理时间。 Flink支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。
容错机制：Spark的CheckPoint只能保证数据不丢失，但是无法保证不重复，Flink使用两阶段提交来处理这个问题。


Flink的组件栈？
四层：
部署层，设计到Flink的部署模式。
RunTime层：提供了Flink的核心计算功能，比如有向无环图的映射、调度等等
API层：实现了面向流处理和批处理的API
Libraries层：在API纸上构建的满足特定应用的计算框架，如：机器学习库，CEP等

你们的Flink集群规模多大？
我们公司是Flink On Yarn 7台
Flink可以支持小集群和TB节点（上千）上运行Flink任务。

Flink的基础编程模型？
Source - Transformation - dataflows（输出流）

Flink中的分布式缓存吗？如何使用？
Flink实现的分布式缓存和Hadoop有异曲同工之妙。目的是在本地读取文件，并把他放在 taskmanager 节点中，防止task重复拉取。
env.registerCachedFile("file:///path/to/exec/file", "localExecFile", true)

说说Flink中的广播变量，使用时需要注意什么？ 
BroadcastPartitioner ：广播分区会将上游数据输出到下游算子的每个实例中。适合于大数据集和小数据集做Jion的场景。
我们知道Flink是并行的，计算过程可能不在一个Slot中进行，那么有一种情况即：当我们需要访问同一份数据。那么Flink中的广播变量就是为了解决这种情况。我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。

Flink窗口的划分：
time-window：有重叠的时间窗口，无重叠。。
count-window：有重叠的数量窗口，无重叠。。。

Flink中的时间分类：事件时间，处理时间，注册时间

Flink是如果做到容错的？
Flink实现容错主要靠强大的CheckPoint机制和State机制。Checkpoint 负责定时制作分布式快照、对程序中的状态进行备份；State 用来存储计算过程中的中间状态。

Flink分布式快照的原理是什么？
Flink的分布式快照是根据Chandy-Lamport算法量身定做的。简单来说就是持续创建分布式数据流及其状态的一致快照。
核心思想是在 input source 端插入 barrier，控制 barrier 的同步来实现 snapshot 的备份和 exactly-once 语义。
要求 比如说kafka source端需要支持可重置杏，sink端需要支持事务，预提交事务，所有barrier对齐以后，提交状态jobmanager通知提交事务。commit

Flink如何保证精准一次语义的？
Flink通过实现两阶段提交和状态保存来实现端到端的一致性语义。 分为以下几个步骤：
开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面
预提交（preCommit）将内存中缓存的数据写入文件并关闭
正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟
丢弃（abort）丢弃临时文件
若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。

说说Flink的内存管理？
Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上。此外，Flink大量的使用了堆外内存。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。Flink 为了直接操作二进制数据实现了自己的序列化框架。

说说Flink的序列化？
Java本身自带的序列化和反序列化的功能，但是辅助信息占用空间比较大，在序列化对象时记录了过多的类信息。摒弃了Java原生的序列化方法，以独特的方式处理数据类型和序列化，包含自己的类型描述符，泛型类型提取和类型序列化框架。TypeInformation 是所有类型描述符的基类。

Flink中的window中出现了数据倾斜，你有什么解决办法？
window产生数据倾斜指的是数据在不同的窗口内堆积的数据量相差过多。本质上产生这种情况的原因是数据源头发送的数据量速度不同导致的。出现这种情况一般通过两种方式来解决：
在数据进入窗口前做预聚合
重新设计窗口聚合的key

Flink任务延迟高，想解决这个问题，你会如何入手？
1、查看checkpoint 是否太频繁，考虑是否可以增大检查点时间
2、是否是触发了Flink的反压机制，如果是就需要调整堆内存大小来优化
3、增加并发数

Flink的反压和Strom有哪些不同？
Strom是通过监控来接收队列负载情况，如果超过高水位就将反压信息写入ZK，ZK会通知所有的Wroker进入反压状态。
Flink的反压是从下游向上游降速，而Strom是从源头降速。

Flink中的算子链？
上下游的并行度一致
下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）
上下游节点都在同一个 slot group 中（下面会解释 slot group）
用户没有禁用 chain

说说Flink1.9的新特性？
支持hive读写、支持UDF
FlinkSQL TopN和Group By等优化
CheckPoint和SavePoint针对实际业务场景做了优化
Flink State查询

消费kafka数据时，如何清理脏数据？
加filter算子

JobManger在集群中扮演了什么角色？
YARN调度的作用，还有负责确认CheckPoint完成通知、释放资源

Flink中水位线是什么？
Watermark 是事件时间域中衡量输入完成进度的一种时间概念，假如当前系统的 watermark 为时间 T，那么系统认为所有事件时间小于T的消息都已经到达，即系统任务它不会再接收到事件时间小于 T的消息了。
- Watermark是一种衡量EventTime进展的机制，可以设定延迟触发。
- Watermark是用于处理乱序时间的，而正确的处理乱序时间，通常用Watermark机制结合window来实现。
- 数据流中的Watermark用于表示timestamp小于Watermark的数据都已经到达了，因此window的执行也是由Watermark触发的。
- Watermark用来让程序自己平衡延迟和结果正确性。
- 水位线只是衡量数据迟没迟到，进入窗口的范围无关系

窗口存在的意义是什么？
聚合类的处理 Flink可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。所以Flink引入了窗口概念。
窗口的作用为了周期性的获取数据。就是把传入的原始数据流切分成多个buckets，所有计算都在单一的buckets中进行。窗口（window）就是从 Streaming 到 Batch 的一个桥梁。
窗口带来的问题是什么？
带来的问题：聚合类处理带来了新的问题，比如乱序/延迟。其解决方案就是 Watermark / allowLateNess / sideOutPut 这一组合拳。
Watermark 的作用是防止 数据乱序 / 指定时间，内获取不到全部数据。
allowLateNess 是将窗口关闭时间再延迟一段时间。
sideOutPut 是最后兜底操作，当指定窗口已经彻底关闭后，就会把所有过期延迟数据放到侧输出流，让用户决定如何处理。
总结起来就是说
Windows -----> Watermark -----> allowLateNess -----> sideOutPut  
用Windows把流数据分块处理，用Watermark确定什么时候不再等待更早的数据/触发窗口进行计算，用allowLateNess 将窗口关闭时间再延迟一段时间。用sideOutPut 最后兜底把数据导出到其他地方。

Flink中处理迟到元素？
1、抛弃
2.发送到侧输出
3、等待迟到元素
1）Flink中当开窗时，迟到是否与水位线是否没过窗口相关。不开窗时，和水位线相关。
2）窗口关闭等待看水位线
3）窗口真关闭看水位线 > 窗口结束时间 + 最大允许迟到时间
4）当数据来了以后，落在窗口的范围只和事件时间有关，即在开窗时，事件时间决定归属在哪个窗口范围内。
5）在开窗时，水位线决定窗口关闭时间。事件时间决定数据所在的窗口范围。
6）sideOutputLateData收集的是未能进入窗口的元素。	

在使用键控状态时使用get获取状态的方式并不是值拷贝，而是获取到一个状态句柄。通过状态句柄可以修改基于相同键中的同一个状态。

keyed State状态编程
1）什么函数可以使用getRuntimeContext来定义状态？只有ProcessFunction吗？
> 只要是RichFunction都可以使用getRuntimeContext来定义状态。
> ProcessFunction的顶级父类归属于RichFunction。
> 2）关于无状态算子
> 比如说普通的map函数是没有状态的，无法从map函数中获取状态，比如getRuntimeContext，但是使用RichMapFunction就可以从map操作中获取状态。
> 3）mapWithState和flatMapWithState两个带状态的map，其本质就是RichMap和RichFlatMap的简写版。
> 4）flatMapWithState函数解析：
> 传入输入类型和状态变量，返回输出类型和状态变量。
> (IN,state)=>(OUT,state)
> FlatMapWithState只能定义在keyedStream之后。
> **mapState、filterState、flatMapState是keyedStream独有。**
> **Flink 提供了非常灵活的状态 API，可以认为所有的算子都可以有状态。**
> 5）map/ filter/ flatmap 本来就是无状态的，但是可以通过实现 RichFunction，获取其上下文，从而对状态进行操作。除此之外，还可以使用 flatMapWithState、mapWithState、filterWithState。
> 6）reduce/ aggregate/ window(window+窗口函数) 本来就是有状态的，由 flink 底层直接管理，当然也可以实现 RichFunction 自定义状态。
> 7）ProcessFunction 是一类特殊的函数类，是 process 方法的参数，也实现了 RichFunction 接口，是一类特殊的富函数，ProcessFunction如果不定义状态默认是无状态的。
> 8）DataStream/ KeyedStream/ ConnectedStream/ WindowedStream 等等都可以调用 process 方法，传入的是不同的 ProcessFunction。
> 9）有状态的流失计算，针对每个算子都是可以去定义的。主要使用RichFunction去实现，RichFunction除了有open和close以外还可以获取运行时上下文。可以将定义好的状态句柄取出来，将状态当做一个本地变量来存取。
> 10）**比如说reduceFunction聚合函数，系统内部会帮我们保存上一个元素的状态。但是如果我们的逻辑不仅想要上一次的状态还需要额外多的状态就需要我们自己实现RichFunction保存更多的状态，这就是Flink底层帮我们管理的状态。**

如何开启2PC？
在检查点配置开启精准一次
    //设置检查点模式默认是精准一次
   env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)

   如何实现端到端的精准一次性？
   - Flink内部保证：checkPoint
- Source端：可重新定位数据读取的位置。
- Sink端：从故障恢复时，数据不会重复写入外部系统，即支持幂等写入或事物写入。

幂等写入和事务写入简单介绍一下？
幂等写入：幂等写入是指一个操作，可以重复执行很多次，但结果只会改变一次，也就是说后面的结果就不起作用了。譬如说redis，ES等KV类型数据库。
> 幂等在状态恢复写入的过程中会有短暂的数据不一致状态。可以保证最终的状态一致性。

事务写入：
1）预写日志方式
把结果数据当成状态保存+批量缓存，然后在收到checkPoint完成的通知时一次性写入Sink系统。
每次都需要等到checkPoint完成再去拉取内容，类似批处理按批写入的方式。相当于在Flink和Sink端实现了一个事务，只有FlinkcheckPoint完成Sink端才会去拉取数据。
DataStream API提供了一个模板类：GenericWriteAheadSink，来实现这种事务型Sink。
>优点：简单易于实现，由于数据在状态后端已经做好了缓存所以无论什么Sink系统，都可以用这种方式一批搞定。
>缺点：对Sink端的实时性有影响，如果在预写日志写入到外部系统时如果发生故障二次写入会导致数据重复。
>2）两阶段提交
>对于每个checkPoint，Sink任务（外部系统）会启动一个事务，并将接下来所有接收到的数据添加到事务里。

预写日志和事务写入的区别？
预写日志写入是将两次barrier的数据全部缓存起来，等到检查点操作完成在一批写入。
两阶段提交是在写入时加入事务，Sink继续写入，如果故障全部撤销。
一个checkPoint对应一个事务。 

Flink CEP是什么？
• 复杂事件处理（Complex Event Processing，CEP）。
• Flink CEP 是在 Flink 中实现的复杂事件处理（CEP）库。
• CEP 允许在无休止的事件流中检测事件模式，让我们有机会掌握数据中重要的部分。
• 一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据 —— 满足则的复杂事件。
• 处理事件的规则，被叫做 “模式”（Pattern）

 Flink CEP 提供了 Pattern API，用于对输入流数据进行复杂事件规则定义，用来提取符合规则的事件序列。

#### SparkOOM

![image-20200716211029983](C:\Users\swime\AppData\Roaming\Typora\typora-user-images\image-20200716211029983.png)





#### spark 2.x新特性

> 通讯框架
>
> 移除AKKA，用Netty替换
>
> api:
>
> 出现新的上下文接口：**SparkSession**出现新的上下文接口：**SparkSession**
>
> 统一了DataFrame和DataSet。DataFrame相当于DataSet[Row]，以及DataSet的增强聚合API
>
> 增强了累加器accumulator的功能，支持Web UI，便捷的API，性能更高
>
> ## sql：
>
> 1. 支持SQL2003标准
>
> 2. 支持ansi-sql 和hive ql的sql parser(SQL解析器)
>
> 3. 支持DDL，支持子查询（in/not in 、 exists/ not exists）
>
> ## 性能
>
> 1. 通过**whole-stage-code generation**（全流程代码生成）技术将SparkSQL和DataSet的性能提升了**2~10倍**。（在下一篇博文中会浅谈全流程代码生成技术）
>
>   2. 通过**vectorization**(向量化)技术提升parquet文件的扫描吞吐量
>
> 3. 提升orc文件的读写性能（2.1.1还不支持orc）
>
> 4. 提升catalyst查询优化器的性能
>
> 5. 通过native实现方式提升窗口函数的性能
>
> sparkstreaming：
>
> 基于DStream的API支持kafka0.10版本
>
> ## Spark MLlib
>
> 1. 基于DataFrame的API支持持久化保存、加载模型、Pipeline，支持更多的算法，支持向量和矩阵使用性能更高的序列化机制。
> 2. Spark R支持MLlib算法，包括线性回归、朴素贝叶斯等
> 3. 未来Spark MLlib将主要基于DataSet API来实现，基于RDD和API将转为维护阶段
>
> ## Other
>
>   1. 支持csv文件
>
>   2. 支持hive风格的bucket表
>
>   3. 支持缓存和程序运行的堆外内存管理
>
> 4. 完全移除了对akka的依赖
>
>   5. 使用Scala2.11代替了Scala2.10，要求基于Scala2.11版本进行开发，而不是Scala2.10
>
> 6. Mesos粗粒度模式下，支持启动多个Executor







#### spark累加器

```scala
//long类型
val emptyLineCount: LongAccumulator=sc.longAccumulator
//map类型累加器
class MyAcc extends AccumulatorV2[String, java.util.List[String]]={
    override isZero
    override copy
    override add  
    override reset
    override merge
    override value
}

//注意点 累加器 map类型的 需要继承AccumulateV2[IN,OUT]
//累加器建议只在行动算子中使用，不要在转换算子中使用，比如转换算子做了ck，那么可能会再计算一遍，导致运算结果出错！（ck会重新起一个任务，又走了遍转换操作！）
//自定义acc 需要注册累加器
sc.register(acc,"first")
```

#### spark读取文件的时候如何分区

```
要么是1要么是2

最小分区数 math.min（defaultParallelism,2）//默认分区数和核心数有关


```

#### spark文件切片 //TODO

#### 关于200

spark-sql如果聚合操作以后，那么聚合后的分区默认是200！（小文件问题）（多少分区就有多少文件 ）



#### spark累加器会遇到的坑

使用累加器的时机是在行动算子上使用。不能在转换算子上使用，可能出现使用ck的时候导致累加器再算一遍的情况

spark 滚动窗口 

#### spark双流join

```
订单表和订单详情表

关系：1对多

所以分下面情况

订单表和订单详情表一个批次全到 直接join

订单表先到订单详情表迟到 订单表缓存，设置失效时间30分钟

订单表迟到订单详情表先到，订单详情表缓存，join一次就清除缓存。

总结：订单表缓存 设置30分钟失效（redis） 订单详情表如果没join成功直接缓存。等到join后直接清除缓存。

key的设计 订单表名+订单id+详情id
```

#### 两日销售对比

```
数据源 log日志 flume第一次准实时采集  cannal采集mysql监控mysql  cannal高可用避免单点故障
cannal结构
Entry=====> RowChange=====> Column 
 
Entry
    Header
        logfileName [binlog文件名]
        logfileOffset [binlog position]
        executeTime [发生的变更]
        schemaName 
        tableName
        eventType [insert/update/delete类型]
    entryType   [事务头BEGIN/事务尾END/数据ROWDATA]
    storeValue  [byte数据,可展开，对应的类型为RowChange]    
 
 
RowChange
    isDdl       [是否是ddl变更操作，比如create table/drop table]
    sql     [具体的ddl sql]
    rowDatas    [具体insert/update/delete的变更数据，可为多条，1个binlog event事件可对应多条变更，比如批处理]
        beforeColumns [Column类型的数组]
        afterColumns [Column类型的数组]  
 
    
Column 
    index       
    sqlType     [jdbc type]
    name        [column name]
    isKey       [是否为主键]
    updated     [是否发生过变更]
    isNull      [值是否为null]
    value       [具体的内容，注意为文本]

```

```scala
使用cannal将数据解析后生产到kafka
将kafka数据封装成样例类实时消费写入HBase 
import org.apache.phoenix.spark._ //导入相关依赖
 orderInfoStream.foreachRDD(rdd => {
            rdd.saveToPhoenix(
                "GMALL_ORDER_INFO1128",
 Seq("ID", "PROVINCE_ID", "CONSIGNEE", "ORDER_COMMENT", "CONSIGNEE_TEL", "ORDER_STATUS", "PAYMENT_WAY", "USER_ID", "IMG_URL", "TOTAL_AMOUNT", "EXPIRE_TIME", "DELIVERY_ADDRESS", "CREATE_TIME", "OPERATE_TIME", "TRACKING_NO", "PARENT_ORDER_ID", "OUT_TRADE_NO", "TRADE_BODY", "CREATE_DATE", "CREATE_HOUR"),
   zkUrl = Some("hadoop102,hadoop103,hadoop104:2181")
            )
获取kafka流，map解析json
将数据存入hbase

通过使用JDBC查询数据里面对数据聚合
     
 字段里有个字段叫做create hour 进行每小时统计
     查询时间 今天和明天 进行实时查询

```

#### canal处理数据

```scala
    // 真正的处理数据
    def parseData(rowDataList: util.List[CanalEntry.RowData],
                  tableName: String,
                  eventType: CanalEntry.EventType) = {
        // 计算订单总额 order_info
        if (tableName == "order_info" && eventType == EventType.INSERT && rowDataList != null && rowDataList.size() > 0) {
            sendToKafka(Constant.ORDER_INFO_TOPIC, rowDataList)
        } else if (tableName == "order_detail" && eventType == EventType.INSERT && rowDataList != null && rowDataList.size() > 0) {
            sendToKafka(Constant.ORDER_DETAIL_TOPIC, rowDataList)
        }
    }
    
```

#### canal读取数据

```scala
    def main(args: Array[String]): Unit = {
        // 1. 连接到canal服务器
        // 1.1 canal服务器的地址  canal服务器的端口号
        val address = new InetSocketAddress("hadoop102", 11111)
        val connector: CanalConnector = CanalConnectors.newSingleConnector(address, "example", "", "")
        // 1.2 连接到canal
        connector.connect()
        // 2. 订阅你要处理的具体表 gmall1128下所有的表
        connector.subscribe("gmall1128.*")
        
        // 3. 读取数据, 解析
        while (true) {
            // 一致监听mysql数据变化, 所以这个地方不挺
            // 100表示最多一次拉取由于100条sql导致的数据的变化
            val msg: Message = connector.get(100)
            val entries: util.List[CanalEntry.Entry] = msg.getEntries
            if (entries != null && entries.size() > 0) {
                // 遍历拿到每个entry
                import scala.collection.JavaConversions._
                for (entry <- entries) {
                    // 处理的EntryType应该时刻RowData
                    if (entry != null && entry.hasEntryType && entry.getEntryType == EntryType.ROWDATA) {
                        // 获取storeValue. 每个entry一个
                        val storeValue: ByteString = entry.getStoreValue
                        // 每个storeVales一个RowChange
                        val rowChange: RowChange = RowChange.parseFrom(storeValue)
                        // 每个rowChange中多个RowData. 一个RowData就表示一行数据
                        val rowDataList: util.List[CanalEntry.RowData] = rowChange.getRowDatasList
                        parseData(rowDataList, entry.getHeader.getTableName, rowChange.getEventType)
                    }
                }
            } else {
                println("没有拉倒数据, 2s之后继续拉....")
                Thread.sleep(2000)
            }
        }
    }
```

## Presto

**应用场景：行为数据分析** 一般做查询使用,insert重新插入回hive分区表支持做的不够好！

不支持hive的语法 INSERT overwrite TABLE

如上类似操作只能先删除后insert

使用时间函数时必须转换成**时间戳**

|                    | Kylin                | Presto                  |
| ------------------ | -------------------- | ----------------------- |
| 查询效率           | 亚秒                 | 秒级                    |
| 支持orc/parquet    | 支持orc，支持parquet | 支持orc读写/只读parquet |
| 是否支持RestfulAPI | no                   | NO                      |

![image-20200726010638970](C:\Users\swime\AppData\Roaming\Typora\typora-user-images\image-20200726010638970.png)

**kylin支持RestfulAPI方便前端人员直接发送请求获取数据，提高开发效率！**

#### 优化

##### 合理设置分区（支持不是特别好 特别是insert的时候）

与Hive类似，Presto会根据元数据信息读取分区数据，合理的分区能减少Presto数据读取量，提升查询性能。

#####  使用列式存储

Presto对ORC文件读取做了特定优化，因此在Hive中创建Presto使用的表时，建议采用ORC格式存储。相对于Parquet，Presto对ORC支持更好。

##### 使用压缩

数据压缩可以减少节点间数据传输对IO带宽压力，对于即席查询需要快速解压，建议采用Snappy压缩。

##### 过滤条件

加上分区字段过滤

##### Group by优化

基数大的字段放前面 例如uid sex（uid很多 sex就2个值）

##### Order by时使用Limit

Order by需要扫描数据到单个worker节点进行排序，导致单个worker需要大量内存。如果是查询Top N或者Bottom N，使用limit可减少排序计算和内存压力。

##### 使用Join语句时将大表放在左边

Presto中join的默认算法是broadcast join，即将join左边的表分割到多个worker，然后将join右边的表数据整个复制一份发送到每个worker进行计算。如果右边的表数据量太大，则可能会报内存溢出错误。



## Kylin

### 简介

eBay上海团队捐献给apache社区，一款定位在开源，分布式分析性数据仓库，提供在Hadoop/Spark之上SQL查询接口，以及**多维**分析（OLAP）能力以支持大数据场景亚秒计算！

# 应用思路

#### 实时指标

```

每小时新增用户（为了方便投放广告）

购买用户群体占比（男孩女孩）
购买用户年龄占比（0-3个月。。。。）

实时下单数
实时下单金额

购物车指标
购物车支付转换率

下单买家数
交易成功单数
退款金额

成交金额GMV
销售金额
```

实时预警（sparkstreaming）

```
优惠券
一个设备切换用户领多个优惠券
同一个设备5分钟内3次及以上用不同账号登录并领取优惠券。在登录过程中没有浏览商品。生产一条预警日志

要的结果是（boolean，alert信息）
思路是先获取流map成 mid，loginfo
groupbykey后再map
用变量存放所有的用户，
```

商品交易额排名



广告点击率（从广告进入商品详情页与进入改商品详情页点击数比率）

订单转换率

#### Flink双流Join（实时对账）

```scala
获取2个流，将2个流进行connect再进行process
实现CoProcessFunction[IN1,IN2,OUT]处理逻辑

保存状态变量
lazy val payState=getRuntimeContext.getState(new ValueStateDescriptor[caseclass]("name"))
```

#### flink窗口函数

```
reduce 窗口状态非常小，容易实现。缺点是输入输出中间状态类型是一致的。

aggregate 也是一种增量计算窗口函数，只保存一个中间状态数据。可以解决输入输出中间状态不一致问题。

process要对全量数据都缓存，Flink将某个Key下某个窗口的所有元素都缓存在`Iterable<IN>`中，我们需要对其进行处理，然后用`Collector<OUT>`收集输出。我们可以使用`Context`获取窗口内更多的信息，包括时间、状态、迟到数据发送位置等。
```

### 业务场景

#### 业务举例

```
我们公司市场营销部10人，他们负责与我们重点客户沟通，通过以宝妈身份，来侧面对商品进行推广。

大数据分析这些用户，通过用户画像来对这些用户精准推送一些广告。

例如分析 家庭主妇

通过手机型号品牌 版本号 可以表现收入水平和年龄水平

通过网络模式+定位 可以判断 该用户是否是全职太太。


通过用户使用app每日平均时长 大于30天且活跃 则为忠实用户
```



#### 实时场景

```
为了拉投资
有些指标需要做实时展示
例如 ：
市场营销类的 新增人访问人数 新增注册人数 订单数量
风控类的 好评率 买家评价数等
客户价值指标 客单价 新客单价 消费评率 复购次数
销售转化指标 广告跳转支付率 
总体运营指标 GMV 
```

#### 离线

```
T+1验证实时分析正确性
特定指标分析
日活，周活，月活
沉默用户
本周回流
七天连续3天活跃
三周活跃

```

表设计

```
优惠券领取 这个业务选取
事实是领取这个动作
选取维度，who where，when
优惠券信息，用户信息，地区等
选取优惠券id，优惠券规则，用户，地区，领取时间，使用时间，使用完毕，失效时间等

```

